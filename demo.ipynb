{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nahyun/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.87s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Args' object has no attribute 'top_p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/nahyun/Projects/LLaVA/demo.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m image_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://llava-vl.github.io/static/images/view.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mArgs\u001b[39m\u001b[39m'\u001b[39m, (), {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m\"\u001b[39m: model_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel_base\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m })()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nahyun/Projects/LLaVA/demo.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m eval_model(args)\n",
      "File \u001b[0;32m~/Projects/LLaVA/llava/eval/run_llava.py:120\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    112\u001b[0m stopping_criteria \u001b[39m=\u001b[39m KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\u001b[1;32m    114\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m    115\u001b[0m     output_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    116\u001b[0m         input_ids,\n\u001b[1;32m    117\u001b[0m         images\u001b[39m=\u001b[39mimages_tensor,\n\u001b[1;32m    118\u001b[0m         do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mtemperature \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m         temperature\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mtemperature,\n\u001b[0;32m--> 120\u001b[0m         top_p\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39;49mtop_p,\n\u001b[1;32m    121\u001b[0m         num_beams\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m    122\u001b[0m         max_new_tokens\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmax_new_tokens,\n\u001b[1;32m    123\u001b[0m         use_cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    124\u001b[0m         stopping_criteria\u001b[39m=\u001b[39m[stopping_criteria],\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m input_token_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    128\u001b[0m n_diff_input_output \u001b[39m=\u001b[39m (input_ids \u001b[39m!=\u001b[39m output_ids[:, :input_token_len])\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Args' object has no attribute 'top_p'"
     ]
    }
   ],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "prompt = \"What are the things I should be cautious about when I visit here?\"\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "args = type('Args', (), {\n",
    "    \"model_path\": model_path,\n",
    "    \"model_base\": None,\n",
    "    \"model_name\": get_model_name_from_path(model_path),\n",
    "    \"query\": prompt,\n",
    "    \"conv_mode\": None,\n",
    "    \"image_file\": image_file,\n",
    "    \"sep\": \",\",\n",
    "    \"temperature\": 1,\n",
    "})()\n",
    "\n",
    "eval_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-20 21:19:27 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='liuhaotian/llava-v1.5-13b', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=True)\n",
      "2023-11-20 21:19:27 | INFO | model_worker | Loading the model llava-v1.5-13b on worker 2e22fc ...\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  33%|██████            | 1/3 [00:19<00:38, 19.19s/it]\n",
      "Loading checkpoint shards:  67%|████████████      | 2/3 [00:38<00:19, 19.05s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:50<00:00, 15.84s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:50<00:00, 16.72s/it]\n",
      "2023-11-20 21:20:19 | ERROR | stderr | \n",
      "2023-11-20 21:20:22 | INFO | model_worker | Register to controller\n",
      "2023-11-20 21:20:22 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m19491\u001b[0m]\n",
      "2023-11-20 21:20:22 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "2023-11-20 21:20:22 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "2023-11-20 21:20:22 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:40000\u001b[0m (Press CTRL+C to quit)\n",
      "2023-11-20 21:20:37 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\n",
      "2023-11-20 21:20:52 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\n",
      "2023-11-20 21:21:07 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\n",
      "2023-11-20 21:21:22 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\n",
      "2023-11-20 21:21:37 | INFO | model_worker | Send heart beat. Models: ['llava-v1.5-13b']. Semaphore: None. global_counter: 0\n",
      "^C\n",
      "2023-11-20 21:21:42 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Shutting down\n",
      "2023-11-20 21:21:42 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "2023-11-20 21:21:42 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
      "2023-11-20 21:21:42 | ERROR | stderr | \u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m19491\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b --load-4bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
